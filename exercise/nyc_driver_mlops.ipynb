{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Feathr Feature Store on Azure Demo Notebook\n","\n","This notebook illustrates the use of Feature Store to create a model that predicts NYC Taxi fares. It includes these steps:\n","\n","\n","This tutorial demonstrates the basic usage of Feathr Feature Store on Azure. It includes these steps:\n","\n","1. Setup Feathr Environment\n","2. Initialize Feathr Client\n","3. Defining Features with Feathr\n","4. Registering Features to the Feathr feature registry\n","5. Creating training data point-in-time correct features joins\n","6. Materializing features to an offline/online store\n","\n","In this tutorial, we use Feathr Feature Store to create a model that predicts NYC Taxi fares. The dataset comes from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). The feature flow is as below:\n","\n","![Feature Flow](https://github.com/linkedin/feathr/blob/main/docs/images/feature_flow.png?raw=true)"]},{"cell_type":"markdown","metadata":{},"source":["# Setup Feathr Environment"]},{"cell_type":"markdown","metadata":{},"source":["**REQUIRED STEP: Fill in the resource prefix when provisioning the resources**\n","\n","***Prior to running the notebook, make sure you have deployed all required resources in your Azure subscription.*** "]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# replace with your prefix\n","resource_prefix = \"mlopsv2fs\"\n","resource_postfix = \"001\"\n","resource_env = \"dev\"\n","storage_accountname = \"stmlopsv2fs001dev\"\n","storage_containername = \"nyctaxi\""]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["import glob\n","import os\n","import tempfile\n","from datetime import datetime, timedelta\n","from math import sqrt\n","\n","import pandas as pd\n","import pandavro as pdx\n","from feathr import FeathrClient\n","from feathr import BOOLEAN, FLOAT, INT32, ValueType\n","from feathr import Feature, DerivedFeature, FeatureAnchor\n","from feathr import BackfillTime, MaterializationSettings\n","from feathr import FeatureQuery, ObservationSettings\n","from feathr import RedisSink, HdfsSink\n","from feathr import INPUT_CONTEXT, HdfsSource\n","from feathr import WindowAggTransformation\n","from feathr import TypedKey\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from azure.identity import DefaultAzureCredential\n","from azure.keyvault.secrets import SecretClient"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["# Get all the required credentials from Azure Key Vault\n","key_vault_name=\"kv-\"+resource_prefix+\"-\"+resource_postfix+resource_env\n","synapse_workspace_url=\"sy\"+resource_prefix+\"-\"+resource_postfix+resource_env\n","adls_account=\"st\"+resource_prefix+resource_postfix+resource_env\n","adls_fs_name=\"dl\"+resource_prefix+resource_postfix+resource_env\n","key_vault_uri = f\"https://{key_vault_name}.vault.azure.net\"\n","credential = DefaultAzureCredential(exclude_interactive_browser_credential=False)\n","client = SecretClient(vault_url=key_vault_uri, credential=credential)\n","secretName = \"FEATHR-ONLINE-STORE-CONN\"\n","retrieved_secret = client.get_secret(secretName).value\n","\n","# Get redis credentials; This is to parse Redis connection string.\n","redis_port=retrieved_secret.split(',')[0].split(\":\")[1]\n","redis_host=retrieved_secret.split(',')[0].split(\":\")[0]\n","redis_password=retrieved_secret.split(',')[1].split(\"password=\",1)[1]\n","redis_ssl=retrieved_secret.split(',')[2].split(\"ssl=\",1)[1]\n","\n","# Set the resource link\n","os.environ['spark_config__azure_synapse__dev_url'] = f'https://{synapse_workspace_url}.dev.azuresynapse.net'\n","os.environ['spark_config__azure_synapse__pool_name'] = 'spdev'\n","os.environ['spark_config__azure_synapse__workspace_dir'] = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_project'\n","os.environ['online_store__redis__host'] = redis_host\n","os.environ['online_store__redis__port'] = redis_port\n","os.environ['online_store__redis__ssl_enabled'] = redis_ssl\n","os.environ['REDIS_PASSWORD']=redis_password\n","feathr_output_path = f'abfss://{adls_fs_name}@{adls_account}.dfs.core.windows.net/feathr_output'\n","os.environ['FEATURE_REGISTRY__API_ENDPOINT']= f'https://app{resource_prefix+resource_postfix+resource_env}.azurewebsites.net/api/v1'\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["'https://appmlopsv2fs001dev.azurewebsites.net/api/v1'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["os.environ.get('FEATURE_REGISTRY__API_ENDPOINT')"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize Feathr Client"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-01-15 18:21:24.544 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n","2023-01-15 18:21:24.568 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - spark_config__azure_synapse__feathr_runtime_location not found in the config file.\n","2023-01-15 18:21:26.636 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n","2023-01-15 18:21:26.637 | INFO     | feathr.client:__init__:176 - Feathr Client 0.9.0 initialized successfully\n"]}],"source":["config_file_name = \"feathr_config.yaml\" \n","feathr_client = FeathrClient(config_path=config_file_name, credential=credential)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Features with Feathr\n","\n","In Feathr, a feature is viewed as a function, mapping from entity id or key, and timestamp to a feature value. For more details on feature definition, please refer to the [Feathr Feature Definition Guide](https://github.com/linkedin/feathr/blob/main/docs/concepts/feature-definition.md)\n","\n","\n","1. The typed key (a.k.a. entity id) identifies the subject of feature, e.g. a user id, 123.\n","2. The feature name is the aspect of the entity that the feature is indicating, e.g. the age of the user.\n","3. The feature value is the actual value of that aspect at a particular time, e.g. the value is 30 at year 2022.\n"]},{"cell_type":"markdown","metadata":{},"source":["Note that, in some cases, such as features defined on top of request data, may have no entity key or timestamp.\n","It is merely a function/transformation executing against request data at runtime.\n","For example, the day of week of the request, which is calculated by converting the request UNIX timestamp.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Define Sources Section with UDFs\n","A feature source is needed for anchored features that describes the raw data in which the feature values are computed from. See the python documentation to get the details on each input column.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["# define a pre-processing function/UDF to be used on spark \n","from pyspark.sql import SparkSession, DataFrame\n","def feathr_udf_day_calc(df: DataFrame) -> DataFrame:\n","    from pyspark.sql.functions import dayofweek, dayofyear, col\n","    df = df.withColumn(\"fare_amount_cents\", col(\"fare_amount\")*100)\n","    return df\n","\n","# define the data source\n","batch_source = HdfsSource(name=\"nycTaxiBatchSource\",\n","                          path=\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/nyc_taxi.parquet\".format(storage_account=storage_accountname, container_name=storage_containername),\n","                          event_timestamp_column=\"lpep_dropoff_datetime\",\n","                          preprocessing=feathr_udf_day_calc,\n","                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\")"]},{"cell_type":"markdown","metadata":{},"source":["### Define Anchors and Features\n","A feature is called an anchored feature when the feature is directly extracted from the source data, rather than computed on top of other features. The latter case is called derived feature."]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["# define anchor features\n","f_trip_distance = Feature(name=\"f_trip_distance\",\n","                          feature_type=FLOAT, transform=\"trip_distance\")\n","f_trip_time_duration = Feature(name=\"f_trip_time_duration\",\n","                               feature_type=INT32,\n","                               transform=\"(to_unix_timestamp(lpep_dropoff_datetime) - to_unix_timestamp(lpep_pickup_datetime))/60\")\n","\n","features = [\n","    f_trip_distance,\n","    f_trip_time_duration,\n","    Feature(name=\"f_is_long_trip_distance\",\n","            feature_type=BOOLEAN,\n","            transform=\"cast_float(trip_distance)>30\"),\n","    Feature(name=\"f_day_of_week\",\n","            feature_type=INT32,\n","            transform=\"dayofweek(lpep_dropoff_datetime)\"),\n","]\n","\n","request_anchor = FeatureAnchor(name=\"request_features\",\n","                               source=INPUT_CONTEXT,\n","                               features=features)"]},{"cell_type":"markdown","metadata":{},"source":["### Window aggregation features\n","\n","For window aggregation features, see the supported fields below:\n","\n","Note that the `agg_func` should be any of these:\n","\n","| Aggregation Type | Input Type | Description |\n","| --- | --- | --- |\n","|SUM, COUNT, MAX, MIN, AVG\t|Numeric|Applies the the numerical operation on the numeric inputs. |\n","|MAX_POOLING, MIN_POOLING, AVG_POOLING\t| Numeric Vector | Applies the max/min/avg operation on a per entry bassis for a given a collection of numbers.|\n","|LATEST| Any |Returns the latest not-null values from within the defined time window |\n","\n","\n","After you have defined features and sources, bring them together to build an anchor:\n","\n","\n","Note that if the data source is from the observation data, the `source` section should be `INPUT_CONTEXT` to indicate the source of those defined anchors."]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["location_id = TypedKey(key_column=\"DOLocationID\",\n","                       key_column_type=ValueType.INT32,\n","                       description=\"location id in NYC\",\n","                       full_name=\"nyc_taxi.location_id\")\n","\n","# calculate the average trip fare, maximum fare and total fare per location for 90 days\n","agg_features = [Feature(name=\"f_location_avg_fare\",\n","                        key=location_id,\n","                        feature_type=FLOAT,\n","                        transform=WindowAggTransformation(agg_expr=\"cast_float(fare_amount)\",\n","                                                          agg_func=\"AVG\",\n","                                                          window=\"90d\")),\n","                Feature(name=\"f_location_max_fare\",\n","                        key=location_id,\n","                        feature_type=FLOAT,\n","                        transform=WindowAggTransformation(agg_expr=\"cast_float(fare_amount)\",\n","                                                          agg_func=\"MAX\",\n","                                                          window=\"90d\")),\n","                Feature(name=\"f_location_total_fare_cents\",\n","                        key=location_id,\n","                        feature_type=FLOAT,\n","                        transform=WindowAggTransformation(agg_expr=\"fare_amount_cents\",\n","                                                          agg_func=\"SUM\",\n","                                                          window=\"90d\")),\n","                ]\n","\n","agg_anchor = FeatureAnchor(name=\"aggregationFeatures\",\n","                           source=batch_source,\n","                           features=agg_features)"]},{"cell_type":"markdown","metadata":{},"source":["### Derived Features \n","Derived features are the features that are computed from other features. They could be computed from anchored features, or other derived features."]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["f_trip_time_distance = DerivedFeature(name=\"f_trip_time_distance\",\n","                                      feature_type=FLOAT,\n","                                      input_features=[\n","                                          f_trip_distance, f_trip_time_duration],\n","                                      transform=\"f_trip_distance * f_trip_time_duration\")\n","\n","f_trip_time_rounded = DerivedFeature(name=\"f_trip_time_rounded\",\n","                                     feature_type=INT32,\n","                                     input_features=[f_trip_time_duration],\n","                                     transform=\"f_trip_time_duration % 10\")\n"]},{"cell_type":"markdown","metadata":{},"source":["And then we need to build those features so that it can be consumed later. Note that we have to build both the \"anchor\" and the \"derived\" features (which is not anchored to a source)."]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["feathr_client.build_features(anchor_list=[agg_anchor, request_anchor], derived_feature_list=[\n","                      f_trip_time_distance, f_trip_time_rounded])"]},{"cell_type":"markdown","metadata":{},"source":["### Registering features\n","\n","We can also register the features with an Apache Atlas compatible service, such as Azure Purview, and share the registered features across teams:"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-01-15 18:38:19.516 | INFO     | feathr.registry._feathr_registry_client:register_features:126 - Check project lineage by this link: https://appmlopsv2fs001dev.azurewebsites.net/projects/nyctaxifs/lineage\n"]}],"source":["feathr_client.register_features()"]},{"cell_type":"markdown","metadata":{},"source":["We can now list the registered features:"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["[{'name': 'f_trip_time_distance',\n","  'id': 'bd382c8f-c2d4-4c35-a899-5fce980f22da',\n","  'qualifiedName': 'nyctaxifs__f_trip_time_distance'},\n"," {'name': 'f_trip_time_rounded',\n","  'id': '67454f48-c0cb-4d2d-af22-40bbac6fea8e',\n","  'qualifiedName': 'nyctaxifs__f_trip_time_rounded'},\n"," {'name': 'f_is_long_trip_distance',\n","  'id': 'bb477734-fe36-4df8-9771-c8e79b39c19c',\n","  'qualifiedName': 'nyctaxifs__request_features__f_is_long_trip_distance'},\n"," {'name': 'f_day_of_week',\n","  'id': '8d99c1a9-2319-4745-afc3-c3d91d108af1',\n","  'qualifiedName': 'nyctaxifs__request_features__f_day_of_week'},\n"," {'name': 'f_trip_distance',\n","  'id': '1377824b-0ea5-4a8f-90ae-8e01a218d1d9',\n","  'qualifiedName': 'nyctaxifs__request_features__f_trip_distance'},\n"," {'name': 'f_trip_time_duration',\n","  'id': '042243d7-130d-4b9f-aebf-dc1039b6f0d2',\n","  'qualifiedName': 'nyctaxifs__request_features__f_trip_time_duration'},\n"," {'name': 'f_location_total_fare_cents',\n","  'id': '8568689c-586e-4348-9c16-3186bf53b1e0',\n","  'qualifiedName': 'nyctaxifs__aggregationFeatures__f_location_total_fare_cents'},\n"," {'name': 'f_location_avg_fare',\n","  'id': '63dcfcad-9b34-45fc-ac26-499099d7e51a',\n","  'qualifiedName': 'nyctaxifs__aggregationFeatures__f_location_avg_fare'},\n"," {'name': 'f_location_max_fare',\n","  'id': '02713195-a370-4d33-97bf-cc275af2326b',\n","  'qualifiedName': 'nyctaxifs__aggregationFeatures__f_location_max_fare'}]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["feathr_client.list_registered_features(project_name=\"nyctaxifs\")"]},{"cell_type":"markdown","metadata":{},"source":["## Create training data using point-in-time correct feature join (e.g usage path: ML model training and scoring)\n","\n","A training dataset usually contains entity id columns, multiple feature columns, event timestamp column and label/target column. \n","\n","To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n","what features and how these features should be joined to the observation data. \n","\n","To learn more on this topic, please refer to [Point-in-time Correctness](https://github.com/linkedin/feathr/blob/main/docs/concepts/point-in-time-join.md)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"scrolled":true,"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-01-15 18:22:21.375 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:67 - Uploading /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feathr_pyspark_driver.py to cloud..\n","2023-01-15 18:22:21.376 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file feathr_pyspark_driver.py\n","2023-01-15 18:22:21.989 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feathr_pyspark_driver.py is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/feathr_pyspark_driver.py\n","2023-01-15 18:22:21.990 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:71 - /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feathr_pyspark_driver.py is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/feathr_pyspark_driver.py\n","2023-01-15 18:22:22.001 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:67 - Uploading /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_join_conf/feature_join.conf to cloud..\n","2023-01-15 18:22:22.001 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file feature_join.conf\n","2023-01-15 18:22:22.608 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_join_conf/feature_join.conf is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/feature_join.conf\n","2023-01-15 18:22:22.610 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:71 - /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_join_conf/feature_join.conf is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/feature_join.conf\n","2023-01-15 18:22:22.610 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:67 - Uploading /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_conf/ to cloud..\n","2023-01-15 18:22:22.612 | INFO     | feathr.spark_provider._synapse_submission:upload_file_to_workdir:400 - Uploading folder /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_conf/\n","2023-01-15 18:22:22.615 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file auto_generated_request_features.conf\n","2023-01-15 18:22:23.197 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - /private/var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_conf/auto_generated_request_features.conf is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/auto_generated_request_features.conf\n","2023-01-15 18:22:23.199 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file auto_generated_anchored_features.conf\n","2023-01-15 18:22:23.780 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - /private/var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_conf/auto_generated_anchored_features.conf is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/auto_generated_anchored_features.conf\n","2023-01-15 18:22:23.781 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file auto_generated_derived_features.conf\n","2023-01-15 18:22:24.536 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - /private/var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_conf/auto_generated_derived_features.conf is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/auto_generated_derived_features.conf\n","2023-01-15 18:22:24.539 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:71 - /var/folders/f7/48b3wb2x7g3_s3pk0cxdp9z40000gn/T/tmp686sskc8/feature_conf/ is uploaded to location: abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/auto_generated_request_features.conf,abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/auto_generated_anchored_features.conf,abfss://dlmlopsv2fs001dev@stmlopsv2fs001dev.dfs.core.windows.net/feathr_project/auto_generated_derived_features.conf\n","2023-01-15 18:22:24.540 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_ACCOUNT is not set in the environment variables.\n","2023-01-15 18:22:24.541 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_KEY is not set in the environment variables.\n","2023-01-15 18:22:24.541 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n","2023-01-15 18:22:24.542 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n","2023-01-15 18:22:24.543 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:118 - Main JAR file is not set, using default package 'com.linkedin.feathr:feathr_2.12:0.9.0' from Maven\n","2023-01-15 18:22:26.846 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:167 - See submitted job here: https://web.azuresynapse.net/en-us/monitoring/sparkapplication\n","2023-01-15 18:22:27.030 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: not_started\n","2023-01-15 18:22:57.192 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: not_started\n","2023-01-15 18:23:27.355 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: not_started\n","2023-01-15 18:23:57.515 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: not_started\n","2023-01-15 18:24:27.707 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: starting\n","2023-01-15 18:24:57.898 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: starting\n","2023-01-15 18:25:28.076 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: starting\n","2023-01-15 18:25:58.255 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: starting\n","2023-01-15 18:26:28.448 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: running\n","2023-01-15 18:26:58.617 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: running\n","2023-01-15 18:27:28.808 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: running\n","2023-01-15 18:27:58.979 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: success\n"]}],"source":["\n","output_path = feathr_output_path\n","\n","# retrieve a list of features with help of the common key\n","feature_query = FeatureQuery(\n","    feature_list=[\"f_location_avg_fare\", \"f_trip_time_rounded\", \"f_is_long_trip_distance\", \"f_location_total_fare_cents\"], key=location_id)\n","\n","# Time settings of the observation data. Used in feature join (so the data useed for join is this data in observeration path combined with feature list described above)\n","settings = ObservationSettings(\n","    observation_path=\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/nyc_taxi.parquet\".format(storage_account=storage_accountname, container_name=storage_containername),\n","    event_timestamp_column=\"lpep_dropoff_datetime\",\n","    timestamp_format=\"yyyy-MM-dd HH:mm:ss\")\n","feathr_client.get_offline_features(observation_settings=settings,\n","                            feature_query=feature_query,\n","                            output_path=output_path)\n","feathr_client.wait_job_to_finish(timeout_sec=500)"]},{"cell_type":"markdown","metadata":{},"source":["Download the result and show the result\n","\n","Let's use the helper function `get_result_df` to download the result and view it:"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-29 11:08:46.241 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: success\n","2022-11-29 11:08:46.309 | INFO     | feathr.spark_provider._synapse_submission:download_file:430 - Beginning reading of results from abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_output\n","Downloading result files: 100%|██████████| 146/146 [00:10<00:00, 13.70it/s]\n","2022-11-29 11:08:57.444 | INFO     | feathr.spark_provider._synapse_submission:download_file:459 - Finish downloading files from abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_output to C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmp5c3n34i9.\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>trip_id</th>\n","      <th>VendorID</th>\n","      <th>lpep_pickup_datetime</th>\n","      <th>lpep_dropoff_datetime</th>\n","      <th>store_and_fwd_flag</th>\n","      <th>RatecodeID</th>\n","      <th>PULocationID</th>\n","      <th>DOLocationID</th>\n","      <th>passenger_count</th>\n","      <th>trip_distance</th>\n","      <th>...</th>\n","      <th>ehail_fee</th>\n","      <th>improvement_surcharge</th>\n","      <th>total_amount</th>\n","      <th>payment_type</th>\n","      <th>trip_type</th>\n","      <th>congestion_surcharge</th>\n","      <th>f_is_long_trip_distance</th>\n","      <th>f_location_total_fare_cents</th>\n","      <th>f_location_avg_fare</th>\n","      <th>f_trip_time_rounded</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>25281</td>\n","      <td>NaN</td>\n","      <td>2020-04-03 07:58:00</td>\n","      <td>2020-04-03 08:26:00</td>\n","      <td>None</td>\n","      <td>NaN</td>\n","      <td>47</td>\n","      <td>125</td>\n","      <td>NaN</td>\n","      <td>12.98</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>32.00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>3170.0</td>\n","      <td>31.700001</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>26724</td>\n","      <td>NaN</td>\n","      <td>2020-04-07 22:45:00</td>\n","      <td>2020-04-07 23:08:00</td>\n","      <td>None</td>\n","      <td>NaN</td>\n","      <td>228</td>\n","      <td>125</td>\n","      <td>NaN</td>\n","      <td>7.13</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>31.58</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>6023.0</td>\n","      <td>30.115002</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6453</td>\n","      <td>2.0</td>\n","      <td>2020-04-08 04:09:53</td>\n","      <td>2020-04-08 04:27:48</td>\n","      <td>N</td>\n","      <td>1.0</td>\n","      <td>75</td>\n","      <td>125</td>\n","      <td>1.0</td>\n","      <td>5.93</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>25.05</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.75</td>\n","      <td>False</td>\n","      <td>7923.0</td>\n","      <td>26.410002</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>8885</td>\n","      <td>1.0</td>\n","      <td>2020-04-10 21:33:57</td>\n","      <td>2020-04-10 22:00:11</td>\n","      <td>N</td>\n","      <td>1.0</td>\n","      <td>89</td>\n","      <td>125</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>29.90</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.5</td>\n","      <td>False</td>\n","      <td>12363.0</td>\n","      <td>24.726002</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>27764</td>\n","      <td>NaN</td>\n","      <td>2020-04-10 21:33:57</td>\n","      <td>2020-04-10 22:00:11</td>\n","      <td>None</td>\n","      <td>NaN</td>\n","      <td>89</td>\n","      <td>125</td>\n","      <td>NaN</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>31.90</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>12363.0</td>\n","      <td>24.726002</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>219</th>\n","      <td>35356</td>\n","      <td>NaN</td>\n","      <td>2020-04-30 16:12:00</td>\n","      <td>2020-04-30 17:16:00</td>\n","      <td>None</td>\n","      <td>NaN</td>\n","      <td>55</td>\n","      <td>119</td>\n","      <td>NaN</td>\n","      <td>25.74</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>68.85</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>337156.0</td>\n","      <td>15.325272</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>220</th>\n","      <td>35424</td>\n","      <td>NaN</td>\n","      <td>2020-04-30 17:05:00</td>\n","      <td>2020-04-30 17:25:00</td>\n","      <td>None</td>\n","      <td>NaN</td>\n","      <td>242</td>\n","      <td>119</td>\n","      <td>NaN</td>\n","      <td>6.89</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>21.27</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>338978.0</td>\n","      <td>15.338370</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>221</th>\n","      <td>24371</td>\n","      <td>2.0</td>\n","      <td>2020-04-30 18:09:34</td>\n","      <td>2020-04-30 18:20:12</td>\n","      <td>N</td>\n","      <td>5.0</td>\n","      <td>74</td>\n","      <td>119</td>\n","      <td>1.0</td>\n","      <td>2.98</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>13.30</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>340278.0</td>\n","      <td>15.327837</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>222</th>\n","      <td>35458</td>\n","      <td>NaN</td>\n","      <td>2020-04-30 18:13:00</td>\n","      <td>2020-04-30 18:25:00</td>\n","      <td>None</td>\n","      <td>NaN</td>\n","      <td>127</td>\n","      <td>119</td>\n","      <td>NaN</td>\n","      <td>2.39</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>13.54</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>False</td>\n","      <td>341327.0</td>\n","      <td>15.306143</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>223</th>\n","      <td>24452</td>\n","      <td>2.0</td>\n","      <td>2020-04-30 21:29:56</td>\n","      <td>2020-04-30 21:30:33</td>\n","      <td>N</td>\n","      <td>1.0</td>\n","      <td>119</td>\n","      <td>119</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>None</td>\n","      <td>0.3</td>\n","      <td>3.80</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>False</td>\n","      <td>341577.0</td>\n","      <td>15.248972</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>35612 rows × 25 columns</p>\n","</div>"],"text/plain":["     trip_id VendorID lpep_pickup_datetime lpep_dropoff_datetime  \\\n","0      25281      NaN  2020-04-03 07:58:00   2020-04-03 08:26:00   \n","1      26724      NaN  2020-04-07 22:45:00   2020-04-07 23:08:00   \n","2       6453      2.0  2020-04-08 04:09:53   2020-04-08 04:27:48   \n","3       8885      1.0  2020-04-10 21:33:57   2020-04-10 22:00:11   \n","4      27764      NaN  2020-04-10 21:33:57   2020-04-10 22:00:11   \n","..       ...      ...                  ...                   ...   \n","219    35356      NaN  2020-04-30 16:12:00   2020-04-30 17:16:00   \n","220    35424      NaN  2020-04-30 17:05:00   2020-04-30 17:25:00   \n","221    24371      2.0  2020-04-30 18:09:34   2020-04-30 18:20:12   \n","222    35458      NaN  2020-04-30 18:13:00   2020-04-30 18:25:00   \n","223    24452      2.0  2020-04-30 21:29:56   2020-04-30 21:30:33   \n","\n","    store_and_fwd_flag RatecodeID  PULocationID  DOLocationID passenger_count  \\\n","0                 None        NaN            47           125             NaN   \n","1                 None        NaN           228           125             NaN   \n","2                    N        1.0            75           125             1.0   \n","3                    N        1.0            89           125             1.0   \n","4                 None        NaN            89           125             NaN   \n","..                 ...        ...           ...           ...             ...   \n","219               None        NaN            55           119             NaN   \n","220               None        NaN           242           119             NaN   \n","221                  N        5.0            74           119             1.0   \n","222               None        NaN           127           119             NaN   \n","223                  N        1.0           119           119             1.0   \n","\n","     trip_distance  ...  ehail_fee  improvement_surcharge  total_amount  \\\n","0            12.98  ...       None                    0.3         32.00   \n","1             7.13  ...       None                    0.3         31.58   \n","2             5.93  ...       None                    0.3         25.05   \n","3             0.00  ...       None                    0.3         29.90   \n","4             0.00  ...       None                    0.3         31.90   \n","..             ...  ...        ...                    ...           ...   \n","219          25.74  ...       None                    0.3         68.85   \n","220           6.89  ...       None                    0.3         21.27   \n","221           2.98  ...       None                    0.3         13.30   \n","222           2.39  ...       None                    0.3         13.54   \n","223           0.00  ...       None                    0.3          3.80   \n","\n","     payment_type  trip_type congestion_surcharge  f_is_long_trip_distance  \\\n","0             NaN        NaN                  NaN                    False   \n","1             NaN        NaN                  NaN                    False   \n","2             1.0        1.0                 2.75                    False   \n","3             1.0        1.0                  2.5                    False   \n","4             NaN        NaN                  NaN                    False   \n","..            ...        ...                  ...                      ...   \n","219           NaN        NaN                  NaN                    False   \n","220           NaN        NaN                  NaN                    False   \n","221           2.0        2.0                  0.0                    False   \n","222           NaN        NaN                  NaN                    False   \n","223           2.0        1.0                  0.0                    False   \n","\n","     f_location_total_fare_cents f_location_avg_fare f_trip_time_rounded  \n","0                         3170.0           31.700001                   8  \n","1                         6023.0           30.115002                   3  \n","2                         7923.0           26.410002                   7  \n","3                        12363.0           24.726002                   6  \n","4                        12363.0           24.726002                   6  \n","..                           ...                 ...                 ...  \n","219                     337156.0           15.325272                   4  \n","220                     338978.0           15.338370                   0  \n","221                     340278.0           15.327837                   0  \n","222                     341327.0           15.306143                   2  \n","223                     341577.0           15.248972                   0  \n","\n","[35612 rows x 25 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["def get_result_df(client: FeathrClient) -> pd.DataFrame:\n","    \"\"\"Download the job result dataset from cloud as a Pandas dataframe.\"\"\"\n","    res_url = feathr_client.get_job_result_uri(block=True, timeout_sec=600)\n","    tmp_dir = tempfile.TemporaryDirectory()\n","    feathr_client.feathr_spark_launcher.download_result(result_path=res_url, local_folder=tmp_dir.name)\n","    dataframe_list = []\n","    # assuming the result are in avro format\n","    for file in glob.glob(os.path.join(tmp_dir.name, '*.avro')):\n","        dataframe_list.append(pdx.read_avro(file))\n","    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n","    tmp_dir.cleanup()\n","    return vertical_concat_df\n","\n","df_res = get_result_df(client)\n","df_res"]},{"cell_type":"markdown","metadata":{},"source":["## Materialize feature value into offline/online storage (e.g usage during: ML model inference path)\n","\n","While Feathr can compute the feature value from the feature definition on-the-fly at request time, it can also pre-compute\n","and materialize the feature value to offline and/or online storage. \n","\n","We can push the generated features to the online store like below:\n"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-29 11:10:13.047 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:67 - Uploading C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feathr_pyspark_driver.py to cloud..\n","2022-11-29 11:10:13.050 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file feathr_pyspark_driver.py\n","2022-11-29 11:10:13.417 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feathr_pyspark_driver.py is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/feathr_pyspark_driver.py\n","2022-11-29 11:10:13.420 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:71 - C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feathr_pyspark_driver.py is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/feathr_pyspark_driver.py\n","2022-11-29 11:10:13.422 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - KAFKA_SASL_JAAS_CONFIG is not set in the environment variables.\n","2022-11-29 11:10:13.423 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:67 - Uploading C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_gen_conf\\auto_gen_config_1589925600.0.conf to cloud..\n","2022-11-29 11:10:13.425 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file auto_gen_config_1589925600.0.conf\n","2022-11-29 11:10:14.728 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_gen_conf\\auto_gen_config_1589925600.0.conf is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_gen_config_1589925600.0.conf\n","2022-11-29 11:10:14.731 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:71 - C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_gen_conf\\auto_gen_config_1589925600.0.conf is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_gen_config_1589925600.0.conf\n","2022-11-29 11:10:14.735 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:67 - Uploading C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_conf/ to cloud..\n","2022-11-29 11:10:14.738 | INFO     | feathr.spark_provider._synapse_submission:upload_file_to_workdir:400 - Uploading folder C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_conf/\n","2022-11-29 11:10:14.742 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file auto_generated_anchored_features.conf\n","2022-11-29 11:10:15.343 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - C:\\Users\\risengupta\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_conf\\auto_generated_anchored_features.conf is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_generated_anchored_features.conf\n","2022-11-29 11:10:15.348 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file auto_generated_derived_features.conf\n","2022-11-29 11:10:15.619 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - C:\\Users\\risengupta\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_conf\\auto_generated_derived_features.conf is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_generated_derived_features.conf\n","2022-11-29 11:10:15.622 | INFO     | feathr.spark_provider._synapse_submission:upload_file:412 - Uploading file auto_generated_request_features.conf\n","2022-11-29 11:10:15.907 | INFO     | feathr.spark_provider._synapse_submission:upload_file:418 - C:\\Users\\risengupta\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_conf\\auto_generated_request_features.conf is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_generated_request_features.conf\n","2022-11-29 11:10:15.911 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:71 - C:\\Users\\RISENG~1\\AppData\\Local\\Temp\\tmpu5bgb285\\feature_conf/ is uploaded to location: abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_generated_anchored_features.conf,abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_generated_derived_features.conf,abfss://rsfeathrfs@rsfeathrdls.dfs.core.windows.net/feathr_project/auto_generated_request_features.conf\n","2022-11-29 11:10:15.913 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_ACCOUNT is not set in the environment variables.\n","2022-11-29 11:10:15.917 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_KEY is not set in the environment variables.\n","2022-11-29 11:10:15.918 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n","2022-11-29 11:10:15.921 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n","2022-11-29 11:10:15.970 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__url not found in the config file.\n","2022-11-29 11:10:15.986 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__user not found in the config file.\n","2022-11-29 11:10:15.989 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - MONITORING_DATABASE_SQL_PASSWORD is not set in the environment variables.\n","2022-11-29 11:10:15.991 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:118 - Main JAR file is not set, using default package 'com.linkedin.feathr:feathr_2.12:0.9.0' from Maven\n","2022-11-29 11:10:16.361 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:167 - See submitted job here: https://web.azuresynapse.net/en-us/monitoring/sparkapplication\n","2022-11-29 11:10:16.445 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: not_started\n","2022-11-29 11:10:46.536 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: starting\n","2022-11-29 11:11:16.637 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: starting\n","2022-11-29 11:11:46.741 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: running\n","2022-11-29 11:12:16.875 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: running\n","2022-11-29 11:12:46.969 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:177 - Current Spark job status: success\n"]}],"source":["backfill_time = BackfillTime(start=datetime(\n","    2020, 5, 20), end=datetime(2020, 5, 20), step=timedelta(days=1))\n","\n","redisSink = RedisSink(table_name=\"nyctaxi\")\n","\n","settings = MaterializationSettings(name=\"nycTaxiFeatures\",\n","                                   backfill_time=backfill_time,\n","                                   sinks=[redisSink],\n","                                   feature_names=[\"f_location_avg_fare\", \"f_location_max_fare\"])\n","\n","feathr_client.materialize_features(settings)\n","feathr_client.wait_job_to_finish(timeout_sec=500)\n"]},{"cell_type":"markdown","metadata":{},"source":["We can then get the features from the online store (Redis in our case):\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["{'128': [25.209131240844727, 72.0], '243': [16.404672622680664, 80.0]}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["res = feathr_client.multi_get_online_features(\"nycTaxiFeatures\", [\"128\", \"243\"], [\"f_location_avg_fare\", \"f_location_max_fare\"])\n","res"]}],"metadata":{"kernelspec":{"display_name":"datascience-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"},"vscode":{"interpreter":{"hash":"c38258a30929bf68ab7ec819763744c59e2d20c19762b3aeecb7509a12778cdd"}}},"nbformat":4,"nbformat_minor":4}
